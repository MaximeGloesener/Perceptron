{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importation des librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.utils import shuffle\n",
    "import csv \n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lecture des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(\"data_doc.mat\")\n",
    "X = mat['Xts'].T.toarray()\n",
    "df_y = pd.DataFrame(mat['yts'])\n",
    "y = pd.get_dummies(df_y[0]).to_numpy() # OneHotEncoding\n",
    "X_to_predict = mat['Xvr'].T.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de zeros au total\n",
    "df_X = pd.DataFrame(X)\n",
    "zero_count = (df_X == 0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607355563\n"
     ]
    }
   ],
   "source": [
    "print(zero_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.amax(X)\n",
    "pos = np.unravel_index(np.argmax(X, axis=None), X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max = 549.0\n",
      "pos = (7684, 4757)\n"
     ]
    }
   ],
   "source": [
    "print(f'max = {m}')\n",
    "print(f'pos = {pos}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalisation des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation entre 0 et 1 \n",
    "X = normalize(X)\n",
    "X_to_predict = normalize(X_to_predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split le jeu de données en validation et entraînement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data en entrainement et validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implémentation du réseau**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, mots, classe, method=None, W=None, B=None):\n",
    "        \"\"\"Initialisation des poids et des biais \"\"\"\n",
    "        if method is None:\n",
    "            # init uniform\n",
    "            self.weights = np.random.randn(mots, classe)\n",
    "            self.biases = np.random.randn(classe)\n",
    "        elif method == \"xavier\":\n",
    "            F_in = mots\n",
    "            F_out = classe \n",
    "            limit = np.sqrt(6 / float(F_in + F_out))\n",
    "            self.weights = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))\n",
    "            self.biases = np.random.randn(classe)\n",
    "        elif method == 'pretrained':\n",
    "            # initialiser les poids et les biais avec les valeurs pré-entrainées\n",
    "            self.weights = W\n",
    "            self.biases = B.reshape(20,)\n",
    "\n",
    "            \n",
    "       \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass pour calculer la sortie du réseau\"\"\"\n",
    "        # sortie z = W^t*X+B\n",
    "        z = np.dot(self.weights.T, x) + self.biases\n",
    "        # fct d'activation = sigmoïde\n",
    "        activation = sigmoid(z)\n",
    "        return z, activation\n",
    "\n",
    "\n",
    "    def evaluate_gradient(self, x, y):\n",
    "        \"\"\"Calcul des dérivées partielles de la loss pour un itéré x\"\"\"\n",
    "        # on récupère la valeur de z et act(z) \n",
    "        z, activation = self.forward(x)\n",
    "        # calculer les dérivées partielles pour la couche de sortie par rapport à W et B\n",
    "        delta_b = (activation-y)*sigmoid_prime(z)\n",
    "        delta_w = np.outer(x, delta_b)\n",
    "        return delta_b, delta_w\n",
    "\n",
    "\n",
    "    \n",
    "    def compute_gradient(self, X, y, lr):\n",
    "        \"\"\"Calcul le gradient pour un batch\"\"\"\n",
    "        \n",
    "        # stocker la somme des gradients des x du batch\n",
    "        somme_delta_b = np.zeros(self.biases.shape)\n",
    "        somme_delta_w = np.zeros(self.weights.shape)\n",
    "\n",
    "        for x,y in zip(X,y):\n",
    "            delta_b, delta_w = self.evaluate_gradient(x, y)\n",
    "            somme_delta_b+=delta_b\n",
    "            somme_delta_w+=delta_w\n",
    "        \n",
    "        return somme_delta_b, somme_delta_w\n",
    "\n",
    "\n",
    "\n",
    "    def GD(self, X, y, lr, epochs):\n",
    "        \"\"\"Gradient descent\"\"\"\n",
    "        acc = []\n",
    "        losses = []\n",
    "        X, y = shuffle(X,y)\n",
    "        for epoch in range(epochs):\n",
    "            # à chaque époque, on met le gradient une seule fosi à jour\n",
    "            somme_delta_b, somme_delta_w =  self.compute_gradient(X, y, lr)\n",
    "            # update les poids      \n",
    "            self.weights -= (lr/X.shape[0])*somme_delta_w \n",
    "            self.biases -= (lr/X.shape[0])*somme_delta_b\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')            \n",
    "            acc.append(accuracy)\n",
    "            losses.append(loss)\n",
    "        return acc, losses\n",
    "\n",
    "    \n",
    "    def SGD_batch(self, X, y, lr, epochs, batch_size, X_val, y_val):\n",
    "        \"\"\"SGD+mini-batch: pour SGD juste prendre un btach_size = 1\"\"\"\n",
    "        \n",
    "        acc = []\n",
    "        losses = [10**8]\n",
    "        val_losses = [10**8]\n",
    "        for epoch in range(epochs):\n",
    "            # shuffle les données\n",
    "            X, y = shuffle(X,y)\n",
    "            # à chaque époque, on met à jour W et B avec mini-batchs\n",
    "            num_samples = y.shape[0]\n",
    "            ret = num_samples % batch_size\n",
    "            # si taille des données pas divisable par batch size alors resize les données\n",
    "            if ret != 0:\n",
    "                X_resize = X[:len(X)-ret]\n",
    "                y_resize = y[:len(y)-ret]\n",
    "            else:\n",
    "                X_resize = X\n",
    "                y_resize = y\n",
    "            iterations = int(num_samples / batch_size)\n",
    "            print(f\"Nombre d'itérations par époque = {iterations}\")\n",
    "            for i in range(iterations):\n",
    "                # pour chaque mini-batch on veut entrainer le modèle et mettre à jour les poids\n",
    "                start = i * batch_size\n",
    "                end = start+batch_size\n",
    "                X_batch = X_resize[start:end, :]\n",
    "                y_batch = y_resize[start:end, :]\n",
    "                somme_delta_b, somme_delta_w =  self.compute_gradient(X_batch, y_batch, lr)\n",
    "                # update les poids      \n",
    "                self.weights -= (lr/X_batch.shape[0])*somme_delta_w \n",
    "                self.biases -= (lr/X_batch.shape[0])*somme_delta_b\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "            acc.append(accuracy)\n",
    "            # update le learning rate si on a une meilleure loss alors lr/2\n",
    "            if loss>losses[-1]:\n",
    "                lr/=2\n",
    "            losses.append(loss)\n",
    "            val_accuracy, val_loss = self.evaluate_model(X_val, y_val)\n",
    "            print(f'Val accuracy = {val_accuracy}')\n",
    "            print(f'Val loss = {val_loss}')\n",
    "            if val_loss > val_losses[-1]:\n",
    "                flag+=1\n",
    "            else:\n",
    "                flag=0\n",
    "            if flag>10:\n",
    "                print('EARLY STOPPING')\n",
    "                sys.exit(0)\n",
    "            val_losses.append(val_loss)\n",
    "        return acc, losses\n",
    "\n",
    "    def Adam(self, X, y, lr, epochs, batch_size, X_val, y_val):\n",
    "        acc = []\n",
    "        losses = []\n",
    "        # init les paramètres pour Adam\n",
    "        beta_1 = 0.9\n",
    "        beta_2 = 0.9\t\t\t\t\n",
    "        epsilon = 1e-8\n",
    "        # init le vecteur\n",
    "        t = 1\t\t\t\t\t\t\n",
    "        m_dw = m_db = v_dw = v_db = 0 \n",
    "        losses = [10**8]\n",
    "        val_losses = [10**8]\n",
    "        # flag utilisé pour early stopping si val loss >>>\n",
    "        flag = 0    \n",
    "        for epoch in range(epochs):\n",
    "            X, y = shuffle(X,y)\n",
    "            # à chaque époque, on met à jour W et B avec mini-batchs\n",
    "            num_samples = y.shape[0]\n",
    "            ret = num_samples % batch_size\n",
    "            # si taille des données pas divisable par batch size alors resize les données\n",
    "            if ret != 0:\n",
    "                X_resize = X[:len(X)-ret]\n",
    "                y_resize = y[:len(y)-ret]\n",
    "            else:\n",
    "                X_resize=X \n",
    "                y_resize=y\n",
    "\n",
    "            iterations = int(num_samples / batch_size)\n",
    "            print(f\"Nombre d'itérations par époque = {iterations}\")\n",
    "            for i in range(iterations):\n",
    "                # pour chaque mini-batch on veut entrainer le modèle et mettre à jour les poids\n",
    "                start = i * batch_size\n",
    "                end = start+batch_size\n",
    "                X_batch = X_resize[start:end, :]\n",
    "                y_batch = y_resize[start:end, :]\n",
    "                grad_b, grad_w =  self.compute_gradient(X_batch, y_batch, lr)\n",
    "                # adam update\n",
    "                # momentum poids\n",
    "                m_dw = beta_1*m_dw + (1-beta_1)*grad_w\n",
    "                # momentum biais\n",
    "                m_db = beta_1*m_db + (1-beta_1)*grad_b\n",
    "\n",
    "                # poids\n",
    "                v_dw = beta_2*v_dw + (1-beta_2)*(grad_w**2)\n",
    "                # biais\n",
    "                v_db = beta_2*v_db + (1-beta_2)*(grad_b**2)\n",
    "\n",
    "                # correction\n",
    "                m_dw_corr = m_dw/(1-beta_1**t)\n",
    "                m_db_corr = m_db/(1-beta_1**t)\n",
    "                v_dw_corr = v_dw/(1-beta_2**t)\n",
    "                v_db_corr = v_db/(1-beta_2**t)\n",
    "                # update les poids      \n",
    "                self.weights -= (lr/X.shape[0])*(m_dw_corr/(np.sqrt(v_dw_corr)+epsilon))\n",
    "                self.biases -= (lr/X.shape[0])*(m_db_corr/(np.sqrt(v_db_corr)+epsilon))\n",
    "                # update les poids avec la somme des gradients du batch au lieu de la moyenne      \n",
    "                #self.weights -= (lr)*(m_dw_corr/(np.sqrt(v_dw_corr)+epsilon))\n",
    "                #self.biases -= (lr)*(m_db_corr/(np.sqrt(v_db_corr)+epsilon))\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "            acc.append(accuracy)\n",
    "            if loss > losses[-1]:\n",
    "                lr /= 2\n",
    "            losses.append(loss)\n",
    "\n",
    "            val_accuracy, val_loss = self.evaluate_model(X_val, y_val)\n",
    "            print(f'Val accuracy = {val_accuracy}')\n",
    "            print(f'Val loss = {val_loss}')\n",
    "            if val_loss > val_losses[-1]:\n",
    "                flag+=1\n",
    "            else:\n",
    "                flag=0\n",
    "            if flag>10:\n",
    "                print('EARLY STOPPING')\n",
    "                return acc, losses\n",
    "            val_losses.append(val_loss)\n",
    "        return acc, losses\n",
    "\n",
    "\n",
    "    def Momentum(self, X, y, lr, epochs, batch_size, X_val, y_val):\n",
    "        acc = []\n",
    "        losses = []\n",
    "        # init les paramètres pour Adam\n",
    "        beta_1 = 0.9\t\t\t\t\t\t\t\t\t\n",
    "        v_dw = v_db = 0 \n",
    "        losses = [10**8]\n",
    "        val_losses = [10**8]\n",
    "        # flag utilisé pour early stopping si val loss >>>\n",
    "        flag = 0    \n",
    "        for epoch in range(epochs):\n",
    "            X, y = shuffle(X,y)\n",
    "            # à chaque époque, on met à jour W et B avec mini-batchs\n",
    "            num_samples = y.shape[0]\n",
    "            ret = num_samples % batch_size\n",
    "            # si taille des données pas divisable par batch size alors resize les données\n",
    "            if ret != 0:\n",
    "                X_resize = X[:len(X)-ret]\n",
    "                y_resize = y[:len(y)-ret]\n",
    "            else:\n",
    "                X_resize=X \n",
    "                y_resize=y\n",
    "\n",
    "            iterations = int(num_samples / batch_size)\n",
    "            print(f\"Nombre d'itérations par époque = {iterations}\")\n",
    "            for i in range(iterations):\n",
    "                # pour chaque mini-batch on veut entrainer le modèle et mettre à jour les poids\n",
    "                start = i * batch_size\n",
    "                end = start+batch_size\n",
    "                X_batch = X_resize[start:end, :]\n",
    "                y_batch = y_resize[start:end, :]\n",
    "                grad_b, grad_w =  self.compute_gradient(X_batch, y_batch, lr)\n",
    "                # momentum update\n",
    "\n",
    "                # poids\n",
    "                v_dw = beta_1*v_dw + (1-beta_1)*(grad_w**2)\n",
    "                # biais\n",
    "                v_db = beta_1*v_db + (1-beta_1)*(grad_b**2)\n",
    "\n",
    "                # update les poids      \n",
    "                self.weights -= (lr/X.shape[0])*(v_dw)\n",
    "                self.biases -= (lr/X.shape[0])*(v_db)\n",
    "\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "            acc.append(accuracy)\n",
    "            if loss > losses[-1]:\n",
    "                lr /= 2\n",
    "            losses.append(loss)\n",
    "\n",
    "            val_accuracy, val_loss = self.evaluate_model(X_val, y_val)\n",
    "            print(f'Val accuracy = {val_accuracy}')\n",
    "            print(f'Val loss = {val_loss}')\n",
    "            if val_loss > val_losses[-1]:\n",
    "                flag+=1\n",
    "            else:\n",
    "                flag=0\n",
    "            if flag>10:\n",
    "                print('EARLY STOPPING')\n",
    "                return acc, losses\n",
    "            val_losses.append(val_loss)\n",
    "        return acc, losses\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Prediction sur des nouvelles données\"\"\"\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            z, act = self.forward(x)\n",
    "            preds.append(act)\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def evaluate_model(self, X, Y):\n",
    "        \"\"\"Calcule l'accuracy du modèle\n",
    "        X = vecteur qui contient des X d'entrées\n",
    "        Y = vecteur de vraie valeurs\n",
    "        \"\"\"\n",
    "        preds = self.predict(X)\n",
    "        loss = (1/(2*Y.shape[0])) * np.linalg.norm(preds - Y, 'fro') ** 2\n",
    "        predicted = np.array([pred.argmax() for pred in preds])\n",
    "        true_y = np.array([y.argmax() for y in Y])\n",
    "        # si predicted == la vraie valeur (test_y) alors True sinon False \n",
    "        correct = predicted == true_y\n",
    "        accuracy = (correct.sum() / len(correct))\n",
    "        return accuracy, loss\n",
    "\n",
    "    def write_output(self, X):\n",
    "        preds = self.predict(X)\n",
    "        predicted = np.array([np.argmax(pred) for pred in preds])\n",
    "        with open('output.csv' ,'w', newline='') as fout:\n",
    "            writer = csv.writer(fout)\n",
    "            writer.writerow([\"id\", \"class\"])\n",
    "            for i,pred in enumerate(predicted, start=1):\n",
    "                writer.writerow([i, (pred+1)*100+1])\n",
    "\n",
    "        \n",
    "def sigmoid(z):\n",
    "    \"\"\"Fonction d'activation sigmoïde\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Dérivée de la sigmoïde\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(X.shape[1], y.shape[1], 'xavier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.08 GiB for an array with shape (12564, 43586) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acc1, losses2 \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mSGD_batch(X_train, y_train, \u001b[39m0.1\u001b[39;49m , \u001b[39m200\u001b[39;49m,\u001b[39m128\u001b[39;49m, X_val, y_val)\n",
      "Cell \u001b[1;32mIn[9], line 109\u001b[0m, in \u001b[0;36mNet.SGD_batch\u001b[1;34m(self, X, y, lr, epochs, batch_size, X_val, y_val)\u001b[0m\n\u001b[0;32m    106\u001b[0m val_losses \u001b[39m=\u001b[39m [\u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m8\u001b[39m]\n\u001b[0;32m    107\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m    108\u001b[0m     \u001b[39m# shuffle les données\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     X, y \u001b[39m=\u001b[39m shuffle(X,y)\n\u001b[0;32m    110\u001b[0m     \u001b[39m# à chaque époque, on met à jour W et B avec mini-batchs\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     num_samples \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:683\u001b[0m, in \u001b[0;36mshuffle\u001b[1;34m(random_state, n_samples, *arrays)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshuffle\u001b[39m(\u001b[39m*\u001b[39marrays, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, n_samples\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    618\u001b[0m     \u001b[39m\"\"\"Shuffle arrays or sparse matrices in a consistent way.\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \n\u001b[0;32m    620\u001b[0m \u001b[39m    This is a convenience alias to ``resample(*arrays, replace=False)`` to do\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[39m      array([0, 1])\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 683\u001b[0m     \u001b[39mreturn\u001b[39;00m resample(\n\u001b[0;32m    684\u001b[0m         \u001b[39m*\u001b[39;49marrays, replace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, n_samples\u001b[39m=\u001b[39;49mn_samples, random_state\u001b[39m=\u001b[39;49mrandom_state\n\u001b[0;32m    685\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:609\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(replace, n_samples, random_state, stratify, *arrays)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[39m# convert sparse matrices to CSR for row-based indexing\u001b[39;00m\n\u001b[0;32m    608\u001b[0m arrays \u001b[39m=\u001b[39m [a\u001b[39m.\u001b[39mtocsr() \u001b[39mif\u001b[39;00m issparse(a) \u001b[39melse\u001b[39;00m a \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 609\u001b[0m resampled_arrays \u001b[39m=\u001b[39m [_safe_indexing(a, indices) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m arrays]\n\u001b[0;32m    610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(resampled_arrays) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    611\u001b[0m     \u001b[39m# syntactic sugar for the unit argument case\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m resampled_arrays[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:609\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[39m# convert sparse matrices to CSR for row-based indexing\u001b[39;00m\n\u001b[0;32m    608\u001b[0m arrays \u001b[39m=\u001b[39m [a\u001b[39m.\u001b[39mtocsr() \u001b[39mif\u001b[39;00m issparse(a) \u001b[39melse\u001b[39;00m a \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 609\u001b[0m resampled_arrays \u001b[39m=\u001b[39m [_safe_indexing(a, indices) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m    610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(resampled_arrays) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    611\u001b[0m     \u001b[39m# syntactic sugar for the unit argument case\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m resampled_arrays[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:356\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[39mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m    355\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 356\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32mc:\\Users\\Maxime\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:185\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    184\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m--> 185\u001b[0m \u001b[39mreturn\u001b[39;00m array[key] \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m array[:, key]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.08 GiB for an array with shape (12564, 43586) and data type float64"
     ]
    }
   ],
   "source": [
    "acc1, losses2 = net.SGD_batch(X_train, y_train, 0.1 , 200,128, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.write_output(X_to_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28c92afe8325fc0816b2f334f44f38e4b06e562e4c3a673584693ef65fd7d28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
