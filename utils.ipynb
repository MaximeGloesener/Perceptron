{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import csv "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lecture des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(\"data_doc.mat\")\n",
    "X = mat['Xts'].T.toarray()\n",
    "df_y = pd.DataFrame(mat['yts'])\n",
    "y = pd.get_dummies(df_y[0]).to_numpy() # OneHotEncoding\n",
    "X_to_predict = pd.DataFrame(mat['Xvr'].T.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traitement des données**\n",
    "-> normalisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_to_predict = scaler.fit_transform(X_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, mots, classe):\n",
    "        \"\"\"Initialisation des poids et des biais \"\"\"\n",
    "        self.weights = np.random.randn(mots, classe)\n",
    "        self.biases = np.random.randn(classe)\n",
    "        \n",
    "       \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass pour calculer la sortie du réseau\"\"\"\n",
    "        # sortie z = W^t*X+B\n",
    "        z = np.dot(self.weights.T, x) + self.biases\n",
    "        # fct d'activation = sigmoïde\n",
    "        activation = sigmoid(z)\n",
    "        return z, activation\n",
    "\n",
    "\n",
    "    def evaluate_gradient(self, x, y):\n",
    "        \"\"\"Calcul des dérivées partielles de la loss\"\"\"\n",
    "        # on récupère la valeur de z et act(z) \n",
    "        z, activation = self.forward(x)\n",
    "        # calculer les dérivées partielles pour la couche de sortie par rapport à W et B\n",
    "        delta_b = (activation-y)*sigmoid_prime(z)\n",
    "        delta_w = np.outer(x, delta_b)\n",
    "        return delta_b, delta_w\n",
    "\n",
    "\n",
    "    \n",
    "    def update(self, X, y, lr):\n",
    "        \"\"\"Mise à jour des poids et des biais en batch directement\n",
    "           GD -> batch size = taille(X),\n",
    "           SGD -> batch size = 1\n",
    "           SGD mini batch -> batch size = size des batchs voulus\"\"\"\n",
    "        \n",
    "        # stocker la somme des gradients des x du batch\n",
    "        somme_delta_b = np.zeros(self.biases.shape)\n",
    "        somme_delta_w = np.zeros(self.weights.shape)\n",
    "\n",
    "        for x,y in zip(X,y):\n",
    "            delta_b, delta_w = self.evaluate_gradient(x, y)\n",
    "            somme_delta_b+=delta_b\n",
    "            somme_delta_w+=delta_w\n",
    "        \n",
    "        self.weights -= (lr/X.shape[0])*somme_delta_w \n",
    "        self.biases -= (lr/X.shape[0])*somme_delta_b\n",
    "\n",
    "                                                                                                                                  \n",
    "    def SGD(self, X, y, lr, epochs):\n",
    "        \"\"\"SGD batchsize=1\"\"\"\n",
    "        # shuffle les données\n",
    "        X, y = shuffle(X,y)\n",
    "        for epoch in range(epochs):\n",
    "            # à chaque époque on parcourt tous les X, on calcule le gradient et on met à jour les poids et le biais pour chaque X\n",
    "            for i in range(len(X)):\n",
    "                self.update(X[i], y[i], lr)\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "\n",
    "    def GD(self, X, y, lr, epochs):\n",
    "        \"\"\"Gradient descent\"\"\"\n",
    "        X, y = shuffle(X,y)\n",
    "        for epoch in range(epochs):\n",
    "            # à chaque époque, on met le gradient une seule fosi à jour\n",
    "            self.update(X, y, lr)\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "\n",
    "    \n",
    "    def SGD_batch(self, X, y, lr, epochs, batch_size):\n",
    "        \"\"\"SGD+mini-batch\"\"\"\n",
    "        # shuffle les données\n",
    "        X, y = shuffle(X,y)\n",
    "        for epoch in range(epochs):\n",
    "            # à chaque époque, on met à jour W et B avec mini-batchs\n",
    "            num_samples = y.shape[0]\n",
    "            ret = num_samples % batch_size\n",
    "            # si taille des données pas divisable par batch size alors resize les données\n",
    "            if ret != 0:\n",
    "                X_resize = X[:len(X)-ret]\n",
    "                y_resize = y[:len(y)-ret]\n",
    "\n",
    "            iterations = int(num_samples / batch_size)\n",
    "            print(f\"Nombre d'itérations par époque = {iterations}\")\n",
    "            for i in range(iterations):\n",
    "                # pour chaque mini-batch on veut entrainer le modèle et mettre à jour les poids\n",
    "                start = i * batch_size\n",
    "                end = start+batch_size\n",
    "                X_batch = X_resize[start:end, :]\n",
    "                y_batch = y_resize[start:end, :]\n",
    "                self.update(X_batch, y_batch, lr)\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Prediction sur des nouvelles données\"\"\"\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            z, act = self.forward(x)\n",
    "            preds.append(act)\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def evaluate_model(self, X, Y):\n",
    "        \"\"\"Calcule l'accuracy du modèle\n",
    "        test_X = vecteur qui contient des X d'entrées\n",
    "        test_Y = vecteur de vraie valeurs\n",
    "        \"\"\"\n",
    "        preds = self.predict(X)\n",
    "        loss = (1/(2*y.shape[0])) * np.linalg.norm(preds - y, 'fro') ** 2\n",
    "        predicted = np.array([np.argmax(pred) for pred in preds])\n",
    "        true_y = np.array([np.argmax(y) for y in Y])\n",
    "        # si predicted == la vraie valeur (test_y) alors True sinon False \n",
    "        correct = predicted == true_y\n",
    "        accuracy = (correct.sum() / float(Y.size))*100\n",
    "        return accuracy, loss\n",
    "\n",
    "    def write_output(self, X):\n",
    "        preds = self.predict(X)\n",
    "        predicted = np.array([np.argmax(pred) for pred in preds])\n",
    "        with open('output.csv' ,'w', newline='') as fout:\n",
    "            writer = csv.writer(fout)\n",
    "            writer.writerow([\"id\", \"class\"])\n",
    "            for i,pred in enumerate(predicted, start=1):\n",
    "                writer.writerow([i, (pred+1)*100+1])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Fonction d'activation sigmoïde\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Dérivée de la sigmoïde\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(X.shape[1], y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.SGD_batch(X,y, 10, 100, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxime\\AppData\\Local\\Temp\\ipykernel_13756\\329248589.py:141: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "net.write_output(X_to_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28c92afe8325fc0816b2f334f44f38e4b06e562e4c3a673584693ef65fd7d28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
