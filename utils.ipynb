{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.utils import shuffle\n",
    "import csv \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lecture des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(\"data_doc.mat\")\n",
    "X = mat['Xts'].T.toarray()\n",
    "df_y = pd.DataFrame(mat['yts'])\n",
    "y = pd.get_dummies(df_y[0]).to_numpy() # OneHotEncoding\n",
    "X_to_predict = mat['Xvr'].T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Normalisation entre 0 et 1 \n",
    "X = normalize(X)\n",
    "X_to_predict = normalize(X_to_predict)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation autour de 0 moyenne nulle et même variance\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_to_predict = scaler.transform(X_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data en entrainement et validation\n",
    "X_train, y_train, X_val, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, mots, classe, method=None):\n",
    "        \"\"\"Initialisation des poids et des biais \"\"\"\n",
    "        if method is None:\n",
    "            # init uniform\n",
    "            self.weights = np.random.randn(mots, classe)\n",
    "            self.biases = np.random.randn(classe)\n",
    "        elif method == \"xavier\":\n",
    "            F_in = mots\n",
    "            F_out = classe \n",
    "            limit = np.sqrt(6 / float(F_in + F_out))\n",
    "            self.weights = np.random.uniform(low=-limit, high=limit, size=(F_in, F_out))\n",
    "            self.biases = np.random.randn(classe)\n",
    "            \n",
    "       \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass pour calculer la sortie du réseau\"\"\"\n",
    "        # sortie z = W^t*X+B\n",
    "        z = np.dot(self.weights.T, x) + self.biases\n",
    "        # fct d'activation = sigmoïde\n",
    "        activation = sigmoid(z)\n",
    "        return z, activation\n",
    "\n",
    "\n",
    "    def evaluate_gradient(self, x, y):\n",
    "        \"\"\"Calcul des dérivées partielles de la loss pour un itéré x\"\"\"\n",
    "        # on récupère la valeur de z et act(z) \n",
    "        z, activation = self.forward(x)\n",
    "        # calculer les dérivées partielles pour la couche de sortie par rapport à W et B\n",
    "        delta_b = (activation-y)*sigmoid_prime(z)\n",
    "        delta_w = np.outer(x, delta_b)\n",
    "        return delta_b, delta_w\n",
    "\n",
    "\n",
    "    \n",
    "    def compute_gradient(self, X, y, lr):\n",
    "        \"\"\"Calcul le gradient pour un batch\"\"\"\n",
    "        \n",
    "        # stocker la somme des gradients des x du batch\n",
    "        somme_delta_b = np.zeros(self.biases.shape)\n",
    "        somme_delta_w = np.zeros(self.weights.shape)\n",
    "\n",
    "        for x,y in zip(X,y):\n",
    "            delta_b, delta_w = self.evaluate_gradient(x, y)\n",
    "            somme_delta_b+=delta_b\n",
    "            somme_delta_w+=delta_w\n",
    "        \n",
    "        return somme_delta_b, somme_delta_w\n",
    "\n",
    "\n",
    "                                                                                                                                  \n",
    "    def SGD(self, X, y, lr, epochs):\n",
    "        \"\"\"SGD batchsize=1\"\"\"\n",
    "        # shuffle les données\n",
    "        X, y = shuffle(X,y)\n",
    "        for epoch in range(epochs):\n",
    "            # à chaque époque on parcourt tous les X, on calcule le gradient et on met à jour les poids et le biais pour chaque X\n",
    "            for i in range(len(X)):\n",
    "                somme_delta_b, somme_delta_w =  self.compute_gradient(X[i], y[i], lr)\n",
    "                # update les poids      \n",
    "                self.weights -= (lr/X[i].shape[0])*somme_delta_w \n",
    "                self.biases -= (lr/X[i].shape[0])*somme_delta_b\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "\n",
    "    def GD(self, X, y, lr, epochs):\n",
    "        \"\"\"Gradient descent\"\"\"\n",
    "        X, y = shuffle(X,y)\n",
    "        for epoch in range(epochs):\n",
    "            # à chaque époque, on met le gradient une seule fosi à jour\n",
    "            somme_delta_b, somme_delta_w =  self.compute_gradient(X, y, lr)\n",
    "            # update les poids      \n",
    "            self.weights -= (lr/X.shape[0])*somme_delta_w \n",
    "            self.biases -= (lr/X.shape[0])*somme_delta_b\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "\n",
    "    \n",
    "    def SGD_batch(self, X, y, lr, epochs, batch_size):\n",
    "        \"\"\"SGD+mini-batch\"\"\"\n",
    "        # shuffle les données\n",
    "        X, y = shuffle(X,y)\n",
    "        losses = [10**8]\n",
    "        for epoch in range(epochs):\n",
    "            # à chaque époque, on met à jour W et B avec mini-batchs\n",
    "            num_samples = y.shape[0]\n",
    "            ret = num_samples % batch_size\n",
    "            # si taille des données pas divisable par batch size alors resize les données\n",
    "            if ret != 0:\n",
    "                X_resize = X[:len(X)-ret]\n",
    "                y_resize = y[:len(y)-ret]\n",
    "\n",
    "            iterations = int(num_samples / batch_size)\n",
    "            print(f\"Nombre d'itérations par époque = {iterations}\")\n",
    "            for i in range(iterations):\n",
    "                # pour chaque mini-batch on veut entrainer le modèle et mettre à jour les poids\n",
    "                start = i * batch_size\n",
    "                end = start+batch_size\n",
    "                X_batch = X_resize[start:end, :]\n",
    "                y_batch = y_resize[start:end, :]\n",
    "                somme_delta_b, somme_delta_w =  self.compute_gradient(X_batch, y_batch, lr)\n",
    "                # update les poids      \n",
    "                self.weights -= (lr/X_batch.shape[0])*somme_delta_w \n",
    "                self.biases -= (lr/X_batch.shape[0])*somme_delta_b\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "\n",
    "            # update le learning rate si on a une meilleure loss alors lr/2\n",
    "            if loss>losses[-1]:\n",
    "                lr/=2\n",
    "            losses.append(loss)\n",
    "\n",
    "    def Adam(self, X, y, lr, epochs, batch_size, X_val, y_val):\n",
    "        # init les paramètres pour Adam\n",
    "        beta_1 = 0.9\n",
    "        beta_2 = 0.999\t\t\t\t\t\n",
    "        epsilon = 1e-8\n",
    "        # init le vecteur\n",
    "        t = 1\t\t\t\t\t\t\n",
    "        m_dw = m_db = v_dw = v_db = 0 \n",
    "        X, y = shuffle(X,y)\n",
    "        losses = [10**8]\n",
    "        val_losses = [10**8]\n",
    "        # flag utilisé pour early stopping si val loss >>>\n",
    "        flag = 0    \n",
    "        for epoch in range(epochs):\n",
    "            # à chaque époque, on met à jour W et B avec mini-batchs\n",
    "            num_samples = y.shape[0]\n",
    "            ret = num_samples % batch_size\n",
    "            # si taille des données pas divisable par batch size alors resize les données\n",
    "            if ret != 0:\n",
    "                X_resize = X[:len(X)-ret]\n",
    "                y_resize = y[:len(y)-ret]\n",
    "\n",
    "            iterations = int(num_samples / batch_size)\n",
    "            print(f\"Nombre d'itérations par époque = {iterations}\")\n",
    "            for i in range(iterations):\n",
    "                # pour chaque mini-batch on veut entrainer le modèle et mettre à jour les poids\n",
    "                start = i * batch_size\n",
    "                end = start+batch_size\n",
    "                X_batch = X_resize[start:end, :]\n",
    "                y_batch = y_resize[start:end, :]\n",
    "                grad_b, grad_w =  self.compute_gradient(X_batch, y_batch, lr)\n",
    "                # adam update\n",
    "                # momentum poids\n",
    "                m_dw = beta_1*m_dw + (1-beta_1)*grad_w\n",
    "                # momentum biais\n",
    "                m_db = beta_1*m_db + (1-beta_1)*grad_b\n",
    "\n",
    "                # poids\n",
    "                v_dw = beta_2*v_dw + (1-beta_2)*(grad_w**2)\n",
    "                # biais\n",
    "                v_db = beta_2*v_db + (1-beta_2)*(grad_b**2)\n",
    "\n",
    "                # correction\n",
    "                m_dw_corr = m_dw/(1-beta_1**t)\n",
    "                m_db_corr = m_db/(1-beta_1**t)\n",
    "                v_dw_corr = v_dw/(1-beta_2**t)\n",
    "                v_db_corr = v_db/(1-beta_2**t)\n",
    "                # update les poids      \n",
    "                self.weights -= (lr/X.shape[0])*(m_dw_corr/(np.sqrt(v_dw_corr)+epsilon))\n",
    "                self.biases -= (lr/X.shape[0])*(m_db_corr/(np.sqrt(v_db_corr)+epsilon))\n",
    "\n",
    "            # calcul de l'accuracy et de la loss pour cette époque \n",
    "            accuracy, loss = self.evaluate_model(X, y)\n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Accuracy = {accuracy}')\n",
    "            print(f'Loss = {loss}')\n",
    "            if loss > losses[-1]:\n",
    "                lr /= 2\n",
    "            losses.append(loss)\n",
    "\n",
    "            val_accuracy, val_loss = self.evaluate_model(X_val, y_val)\n",
    "            print(f'Val accuracy = {val_accuracy}')\n",
    "            print(f'Val loss = {val_loss}')\n",
    "            if val_loss > val_losses[-1]:\n",
    "                flag+=1\n",
    "            else:\n",
    "                flag=0\n",
    "            if flag>5:\n",
    "                print('EARLY STOPPING')\n",
    "                exit()\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Prediction sur des nouvelles données\"\"\"\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            z, act = self.forward(x)\n",
    "            preds.append(act)\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def evaluate_model(self, X, Y):\n",
    "        \"\"\"Calcule l'accuracy du modèle\n",
    "        X = vecteur qui contient des X d'entrées\n",
    "        Y = vecteur de vraie valeurs\n",
    "        \"\"\"\n",
    "        preds = self.predict(X)\n",
    "        loss = (1/(2*Y.shape[0])) * np.linalg.norm(preds - Y, 'fro') ** 2\n",
    "        predicted = np.array([pred.argmax() for pred in preds])\n",
    "        true_y = np.array([y.argmax() for y in Y])\n",
    "        # si predicted == la vraie valeur (test_y) alors True sinon False \n",
    "        correct = predicted == true_y\n",
    "        accuracy = (correct.sum() / len(correct))\n",
    "        return accuracy, loss\n",
    "\n",
    "    def write_output(self, X):\n",
    "        preds = self.predict(X)\n",
    "        predicted = np.array([np.argmax(pred) for pred in preds])\n",
    "        with open('output.csv' ,'w', newline='') as fout:\n",
    "            writer = csv.writer(fout)\n",
    "            writer.writerow([\"id\", \"class\"])\n",
    "            for i,pred in enumerate(predicted, start=1):\n",
    "                writer.writerow([i, (pred+1)*100+1])\n",
    "\n",
    "        \n",
    "def sigmoid(z):\n",
    "    \"\"\"Fonction d'activation sigmoïde\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Dérivée de la sigmoïde\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(X.shape[1], y.shape[1], 'xavier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'itérations par époque = 218\n",
      "Epoch 0\n",
      "Accuracy = 0.049498567335243555\n",
      "Loss = 1.5860012330108426\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 1\n",
      "Accuracy = 0.0505730659025788\n",
      "Loss = 1.222793948854219\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 2\n",
      "Accuracy = 0.056303724928366765\n",
      "Loss = 1.013846472288801\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 3\n",
      "Accuracy = 0.0771489971346705\n",
      "Loss = 0.8770347408437287\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 4\n",
      "Accuracy = 0.10730659025787966\n",
      "Loss = 0.7804534397973723\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 5\n",
      "Accuracy = 0.1492836676217765\n",
      "Loss = 0.7085670322023333\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 6\n",
      "Accuracy = 0.20222063037249283\n",
      "Loss = 0.6528741509872769\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 7\n",
      "Accuracy = 0.2613896848137536\n",
      "Loss = 0.6083433897325683\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 8\n",
      "Accuracy = 0.32335243553008597\n",
      "Loss = 0.571814932770081\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 9\n",
      "Accuracy = 0.38818051575931234\n",
      "Loss = 0.5412074490218446\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 10\n",
      "Accuracy = 0.451432664756447\n",
      "Loss = 0.5150949829058348\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 11\n",
      "Accuracy = 0.5108882521489971\n",
      "Loss = 0.4924680522256796\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 12\n",
      "Accuracy = 0.5628223495702006\n",
      "Loss = 0.47259221578371424\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 13\n",
      "Accuracy = 0.6100286532951289\n",
      "Loss = 0.4549208491244424\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 14\n",
      "Accuracy = 0.6504297994269341\n",
      "Loss = 0.4390393794588916\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 15\n",
      "Accuracy = 0.6855300859598854\n",
      "Loss = 0.4246284648139255\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 16\n",
      "Accuracy = 0.7152578796561605\n",
      "Loss = 0.41143895849113316\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 17\n",
      "Accuracy = 0.7419770773638968\n",
      "Loss = 0.3992744235712604\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 18\n",
      "Accuracy = 0.7648280802292263\n",
      "Loss = 0.38797861516499216\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 19\n",
      "Accuracy = 0.7815186246418339\n",
      "Loss = 0.3774263115319202\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 20\n",
      "Accuracy = 0.796919770773639\n",
      "Loss = 0.3675164552701734\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 21\n",
      "Accuracy = 0.8097421203438395\n",
      "Loss = 0.35816692785057336\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 22\n",
      "Accuracy = 0.8211318051575931\n",
      "Loss = 0.349310512926658\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 23\n",
      "Accuracy = 0.8292979942693409\n",
      "Loss = 0.3408917527592407\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 24\n",
      "Accuracy = 0.8361031518624642\n",
      "Loss = 0.3328644948013049\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 25\n",
      "Accuracy = 0.8430515759312321\n",
      "Loss = 0.32518998097756724\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 26\n",
      "Accuracy = 0.8480659025787965\n",
      "Loss = 0.31783536538963064\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 27\n",
      "Accuracy = 0.8539398280802293\n",
      "Loss = 0.31077256807041514\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 28\n",
      "Accuracy = 0.8586676217765044\n",
      "Loss = 0.30397738953910053\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 29\n",
      "Accuracy = 0.8626790830945559\n",
      "Loss = 0.2974288258783794\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 30\n",
      "Accuracy = 0.8664756446991404\n",
      "Loss = 0.2911085370878838\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 31\n",
      "Accuracy = 0.8696991404011462\n",
      "Loss = 0.2850004320914952\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 32\n",
      "Accuracy = 0.8731375358166189\n",
      "Loss = 0.27909034189598636\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 33\n",
      "Accuracy = 0.8760744985673352\n",
      "Loss = 0.27336575848875155\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 34\n",
      "Accuracy = 0.8785100286532951\n",
      "Loss = 0.2678156218439273\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 35\n",
      "Accuracy = 0.8810171919770774\n",
      "Loss = 0.2624301414820054\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 36\n",
      "Accuracy = 0.8834527220630373\n",
      "Loss = 0.25720064267325454\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 37\n",
      "Accuracy = 0.8856017191977077\n",
      "Loss = 0.2521194305946823\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 38\n",
      "Accuracy = 0.8876790830945559\n",
      "Loss = 0.24717966844883044\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 39\n",
      "Accuracy = 0.8899713467048711\n",
      "Loss = 0.2423752675875565\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 40\n",
      "Accuracy = 0.8925501432664756\n",
      "Loss = 0.2377007889365679\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 41\n",
      "Accuracy = 0.8949140401146132\n",
      "Loss = 0.23315135548145072\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 42\n",
      "Accuracy = 0.8965616045845273\n",
      "Loss = 0.22872257540598587\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 43\n",
      "Accuracy = 0.8984957020057307\n",
      "Loss = 0.2244104749585081\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 44\n",
      "Accuracy = 0.9001432664756447\n",
      "Loss = 0.2202114396043821\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 45\n",
      "Accuracy = 0.9020773638968481\n",
      "Loss = 0.21612216178232718\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 46\n",
      "Accuracy = 0.9035816618911174\n",
      "Loss = 0.21213959373918\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 47\n",
      "Accuracy = 0.9052292263610315\n",
      "Loss = 0.20826090439991896\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 48\n",
      "Accuracy = 0.9066618911174785\n",
      "Loss = 0.2044834398309411\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 49\n",
      "Accuracy = 0.908810888252149\n",
      "Loss = 0.20080468734926937\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 50\n",
      "Accuracy = 0.9108166189111748\n",
      "Loss = 0.1972222435816212\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 51\n",
      "Accuracy = 0.9126074498567335\n",
      "Loss = 0.1937337867809026\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 52\n",
      "Accuracy = 0.9136819484240688\n",
      "Loss = 0.19033705355842384\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 53\n",
      "Accuracy = 0.9152578796561605\n",
      "Loss = 0.18702982000399768\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 54\n",
      "Accuracy = 0.9167621776504298\n",
      "Loss = 0.18380988701752157\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 55\n",
      "Accuracy = 0.9186962750716332\n",
      "Loss = 0.1806750695820229\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 56\n",
      "Accuracy = 0.9206303724928366\n",
      "Loss = 0.17762318965222196\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 57\n",
      "Accuracy = 0.9216332378223495\n",
      "Loss = 0.17465207229053736\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 58\n",
      "Accuracy = 0.9226361031518625\n",
      "Loss = 0.1717595446391582\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 59\n",
      "Accuracy = 0.9234957020057306\n",
      "Loss = 0.16894343727243294\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 60\n",
      "Accuracy = 0.925\n",
      "Loss = 0.1662015874390278\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 61\n",
      "Accuracy = 0.9255014326647565\n",
      "Loss = 0.16353184368894494\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 62\n",
      "Accuracy = 0.9262893982808023\n",
      "Loss = 0.16093207139146196\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 63\n",
      "Accuracy = 0.9272206303724928\n",
      "Loss = 0.1584001586842089\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 64\n",
      "Accuracy = 0.9281518624641834\n",
      "Loss = 0.15593402244387763\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 65\n",
      "Accuracy = 0.9291547277936962\n",
      "Loss = 0.153531613929869\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 66\n",
      "Accuracy = 0.9301575931232091\n",
      "Loss = 0.15119092382754593\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 67\n",
      "Accuracy = 0.930730659025788\n",
      "Loss = 0.14890998651493173\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 68\n",
      "Accuracy = 0.9316618911174785\n",
      "Loss = 0.14668688348107858\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 69\n",
      "Accuracy = 0.9325931232091691\n",
      "Loss = 0.14451974590430705\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 70\n",
      "Accuracy = 0.9334527220630372\n",
      "Loss = 0.14240675644733664\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 71\n",
      "Accuracy = 0.9343123209169054\n",
      "Loss = 0.14034615036046919\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 72\n",
      "Accuracy = 0.935028653295129\n",
      "Loss = 0.13833621600947604\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 73\n",
      "Accuracy = 0.9358882521489972\n",
      "Loss = 0.1363752949561088\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 74\n",
      "Accuracy = 0.9363180515759313\n",
      "Loss = 0.13446178171350281\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 75\n",
      "Accuracy = 0.9373925501432665\n",
      "Loss = 0.13259412327950762\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 76\n",
      "Accuracy = 0.938538681948424\n",
      "Loss = 0.130770818524191\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 77\n",
      "Accuracy = 0.9395415472779369\n",
      "Loss = 0.1289904174797595\n",
      "Nombre d'itérations par époque = 218\n",
      "Epoch 78\n",
      "Accuracy = 0.9405444126074498\n",
      "Loss = 0.12725152055723696\n",
      "Nombre d'itérations par époque = 218\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maxgl\\maxglo\\gillisOptiPerceptron\\Perceptron\\utils.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m net\u001b[39m.\u001b[39;49mAdam(X,y, \u001b[39m10\u001b[39;49m, \u001b[39m300\u001b[39;49m, \u001b[39m64\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\maxgl\\maxglo\\gillisOptiPerceptron\\Perceptron\\utils.ipynb Cell 7\u001b[0m in \u001b[0;36mNet.Adam\u001b[1;34m(self, X, y, lr, epochs, batch_size)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m X_batch \u001b[39m=\u001b[39m X_resize[start:end, :]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m y_batch \u001b[39m=\u001b[39m y_resize[start:end, :]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m grad_b, grad_w \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradient(X_batch, y_batch, lr)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39m# adam update\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39m# momentum poids\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m m_dw \u001b[39m=\u001b[39m beta_1\u001b[39m*\u001b[39mm_dw \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mbeta_1)\u001b[39m*\u001b[39mgrad_w\n",
      "\u001b[1;32mc:\\Users\\maxgl\\maxglo\\gillisOptiPerceptron\\Perceptron\\utils.ipynb Cell 7\u001b[0m in \u001b[0;36mNet.compute_gradient\u001b[1;34m(self, X, y, lr)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     delta_b, delta_w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_gradient(x, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     somme_delta_b\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mdelta_b\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     somme_delta_w\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mdelta_w\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxgl/maxglo/gillisOptiPerceptron/Perceptron/utils.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mreturn\u001b[39;00m somme_delta_b, somme_delta_w\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.Adam(X,y, 10, 300, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.write_output(X_to_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d14695ea5945e3f4f0ac238f520c0cbdd58c9ea49b8d1a8480368eb73727c69d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
